{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy matplotlib\n",
        "!pip install scikit-learn seaborn\n",
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "lQdp6hW04H7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, precision_score, recall_score, f1_score, accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, get_scheduler"
      ],
      "metadata": {
        "id": "h9fR8W0AHLz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Functions"
      ],
      "metadata": {
        "id": "Kl830xOoGpFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define evaluation function\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            _, predicted_labels = torch.max(logits, dim=1)\n",
        "\n",
        "            predictions.extend(predicted_labels.cpu().numpy())\n",
        "            true_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1\n",
        "    }"
      ],
      "metadata": {
        "id": "Mxt0DClDup20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define pre-processing functions\n",
        "class FrenchDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, tokenizer, max_len=512):\n",
        "        self.sentences = sentences\n",
        "        self.labels = [label_mapping[label] for label in labels]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(sentence, add_special_tokens=True, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "Rq2hvMuF5Dxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a mapping from current string labels to integers\n",
        "label_mapping = {\n",
        "    'A1': 0,\n",
        "    'A2': 1,\n",
        "    'B1': 2,\n",
        "    'B2': 3,\n",
        "    'C1': 4,\n",
        "    'C2': 5\n",
        "}"
      ],
      "metadata": {
        "id": "BTJjopzB5Aog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define pre-processing functions for unlabelled data\n",
        "class UnlabeledFrenchDataset(Dataset):\n",
        "    def __init__(self, sentences, tokenizer, max_len=512):\n",
        "        self.sentences = sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        encoding = self.tokenizer(sentence, add_special_tokens=True, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }"
      ],
      "metadata": {
        "id": "pnWmU7R0O4eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune Bert Model"
      ],
      "metadata": {
        "id": "vWXiXbUyuD68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "url = 'https://raw.githubusercontent.com/RebeccaKessler/Machine_Learning/main/training_data.csv'\n",
        "df = pd.read_csv(url)\n",
        "sentences = df['sentence'].tolist()\n",
        "labels = df['difficulty'].tolist()\n"
      ],
      "metadata": {
        "id": "Y2tKUsYzM2fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased', num_labels=6)"
      ],
      "metadata": {
        "id": "pqsetw8mueZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n",
        "# Split the data into training and validation sets\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize datasets\n",
        "train_dataset = FrenchDataset(train_sentences, train_labels, tokenizer)\n",
        "val_dataset = FrenchDataset(val_sentences, val_labels, tokenizer)\n",
        "\n",
        "# Prepare data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "6yzmXT2_vcZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Set up the training of model\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 4\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ],
      "metadata": {
        "id": "0hu6R4wc5cRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "-xdPuguh5hmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "evaluation_results = evaluate_model(model, val_loader, device)\n",
        "print(\"Accuracy:\", evaluation_results['accuracy'])\n",
        "print(\"Precision:\", evaluation_results['precision'])\n",
        "print(\"Recall:\", evaluation_results['recall'])\n",
        "print(\"F1 Score:\", evaluation_results['f1_score'])\n"
      ],
      "metadata": {
        "id": "5BWVIJ78OAJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-traine on full dataset"
      ],
      "metadata": {
        "id": "7IJG63fzNP5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased', num_labels=6)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "iMbDSRBeu2bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "# Create a dataset with all data\n",
        "full_dataset = FrenchDataset(sentences, labels, tokenizer)\n",
        "\n",
        "# Data loader for the full dataset\n",
        "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "KJE0DXQou5yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 4\n",
        "num_training_steps = num_epochs * len(full_loader)\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ],
      "metadata": {
        "id": "TMpInN08FDbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in full_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "b2q3jJSZFIT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"full_model_directory\")\n",
        "tokenizer.save_pretrained(\"full_model_directory\")"
      ],
      "metadata": {
        "id": "5BfGXN-ZFKOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Predictions"
      ],
      "metadata": {
        "id": "PgXvX5j5vHsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data\n",
        "url = 'https://raw.githubusercontent.com/RebeccaKessler/Machine_Learning/main/unlabelled_test_data.csv'\n",
        "unlabeled_df = pd.read_csv(url)\n",
        "unlabeled_sentences = unlabeled_df['sentence'].tolist()"
      ],
      "metadata": {
        "id": "RVPoKYkWFLec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare unlabeled dataset\n",
        "unlabeled_dataset = UnlabeledFrenchDataset(unlabeled_sentences, tokenizer)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "yAY4mlqDPOmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in unlabeled_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        _, predicted_labels = torch.max(logits, dim=1)\n",
        "        predictions.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "# Map predictions back to label strings\n",
        "predicted_difficulties = [list(label_mapping.keys())[label] for label in predictions]\n",
        "\n",
        "# Create new dataframe with predictions\n",
        "data = {\n",
        "    'sentence': unlabeled_sentences,\n",
        "    'predicted_difficulty': predicted_difficulties\n",
        "}\n",
        "results_df = pd.DataFrame(data)\n",
        "\n",
        "# Save as a CSV\n",
        "results_df.to_csv('predictions_bert.csv', index=False)"
      ],
      "metadata": {
        "id": "rRbn6AyuPRq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}