{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODbyU4JPPu879WQ64qsEjO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RebeccaKessler/Machine_Learning/blob/main/Codes/Combined_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install required packages\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate -U\n",
        "!pip install pandas numpy matplotlib\n",
        "!pip install scikit-learn seaborn\n",
        "!pip install transformers torch pandas scikit-learn\n",
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "ZS0rhKDXmW0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import required packages\n",
        "from transformers import Trainer, TrainingArguments, CamembertTokenizer, CamembertForSequenceClassification, CamembertConfig, FlaubertTokenizer, FlaubertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import pipeline\n",
        "import joblib\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "mAEt4b8tmY1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define compute metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "bb6pid-NmiZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = str(self.data.iloc[idx]['sentence'])\n",
        "        label = int(self.data.iloc[idx]['encoded_labels'])\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "sGGFw4ikmdv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "url = 'https://raw.githubusercontent.com/RebeccaKessler/Machine_Learning/main/training_data.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Define label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder and transform labels\n",
        "data['encoded_labels'] = label_encoder.fit_transform(data['difficulty'])\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "-LZOGCqnmgTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tokenizer\n",
        "tokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased')\n",
        "\n",
        "# K-Fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(data)):\n",
        "    print(f\"Training fold {fold+1}\")\n",
        "    train_data = data.iloc[train_idx]\n",
        "    val_data = data.iloc[val_idx]\n",
        "\n",
        "    train_dataset = CustomDataset(train_data, tokenizer)\n",
        "    eval_dataset = CustomDataset(val_data, tokenizer)\n",
        "\n",
        "    # Set up the Trainer\n",
        "    model = FlaubertForSequenceClassification.from_pretrained('flaubert/flaubert_base_cased', num_labels=6)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_fold_{fold}',\n",
        "        learning_rate=0.00005,\n",
        "        num_train_epochs=7,\n",
        "        per_device_train_batch_size=16,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'./logs_fold_{fold}',\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        fp16=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    accuracy_list.append(eval_result['eval_accuracy'])\n",
        "    precision_list.append(eval_result['eval_precision'])\n",
        "    recall_list.append(eval_result['eval_recall'])\n",
        "    f1_list.append(eval_result['eval_f1'])"
      ],
      "metadata": {
        "id": "f9IfM71vmlZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute overall statistics of the model\n",
        "overall_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "overall_precision = sum(precision_list) / len(precision_list)\n",
        "overall_recall = sum(recall_list) / len(recall_list)\n",
        "overall_f1 = sum(f1_list) / len(f1_list)\n",
        "\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "print(f\"Overall Precision: {overall_precision:.4f}\")\n",
        "print(f\"Overall Recall: {overall_recall:.4f}\")\n",
        "print(f\"Overall F1 Score: {overall_f1:.4f}\")"
      ],
      "metadata": {
        "id": "bQoDkwjrmoeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "url = 'https://raw.githubusercontent.com/RebeccaKessler/Machine_Learning/main/training_data.csv'\n",
        "full_data = pd.read_csv(url)\n",
        "label_encoder = LabelEncoder()\n",
        "full_data['encoded_labels'] = label_encoder.fit_transform(full_data['difficulty'])\n",
        "\n",
        "# Load CamemBERT model pre-trained\n",
        "model = FlaubertForSequenceClassification.from_pretrained('flaubert/flaubert_base_cased', num_labels=6)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased')\n",
        "\n",
        "# Create a new dataset object with the entire data\n",
        "final_dataset = CustomDataset(combined_data, tokenizer)\n",
        "\n",
        "# Modify training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=0.00005,\n",
        "    num_train_epochs=7,\n",
        "    per_device_train_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    fp16=True,\n",
        "    )\n",
        "\n",
        "# Re-initialize and train the Trainer with the new combined dataset\n",
        "final_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=final_dataset,\n",
        "    compute_metrics=None\n",
        ")\n",
        "\n",
        "# Retrain the model on the whole dataset\n",
        "final_trainer.train()\n",
        "\n",
        "# Save the final trained model and tokenizer\n",
        "model.save_pretrained('./flaubert_final_model')\n",
        "tokenizer.save_pretrained('./flaubert_final_model')"
      ],
      "metadata": {
        "id": "aOZxEnI4mrXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "url = 'https://raw.githubusercontent.com/RebeccaKessler/Machine_Learning/main/training_data.csv'\n",
        "data = pd.read_csv(url)\n",
        "label_encoder = LabelEncoder()\n",
        "data['encoded_labels'] = label_encoder.fit_transform(data['difficulty'])"
      ],
      "metadata": {
        "id": "St91pqdV2Tex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "# K-Fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(data)):\n",
        "    print(f\"Training fold {fold+1}\")\n",
        "    train_data = data.iloc[train_idx]\n",
        "    val_data = data.iloc[val_idx]\n",
        "\n",
        "    train_dataset = CustomDataset(train_data, tokenizer)\n",
        "    eval_dataset = CustomDataset(val_data, tokenizer)\n",
        "\n",
        "    # Set up the Trainer\n",
        "    model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_fold_{fold}',\n",
        "        learning_rate=0.00015,\n",
        "        num_train_epochs=7,\n",
        "        per_device_train_batch_size=16,\n",
        "        warmup_steps=1000,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'./logs_fold_{fold}',\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        fp16=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    accuracy_list.append(eval_result['eval_accuracy'])\n",
        "    precision_list.append(eval_result['eval_precision'])\n",
        "    recall_list.append(eval_result['eval_recall'])\n",
        "    f1_list.append(eval_result['eval_f1'])"
      ],
      "metadata": {
        "id": "97J7tYSTonVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute overall statistics of the model\n",
        "overall_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "overall_precision = sum(precision_list) / len(precision_list)\n",
        "overall_recall = sum(recall_list) / len(recall_list)\n",
        "overall_f1 = sum(f1_list) / len(f1_list)\n",
        "\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "print(f\"Overall Precision: {overall_precision:.4f}\")\n",
        "print(f\"Overall Recall: {overall_recall:.4f}\")\n",
        "print(f\"Overall F1 Score: {overall_f1:.4f}\")"
      ],
      "metadata": {
        "id": "Vc8JaCHGosCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "url = 'https://raw.githubusercontent.com/RebeccaKessler/Machine_Learning/main/training_data.csv'\n",
        "full_data = pd.read_csv(url)\n",
        "label_encoder = LabelEncoder()\n",
        "full_data['encoded_labels'] = label_encoder.fit_transform(full_data['difficulty'])\n",
        "\n",
        "#Load CamemBERT model pre-trained\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "# Create a new dataset object with the entire data\n",
        "final_dataset = CustomDataset(full_data, tokenizer)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=0.00015,\n",
        "    num_train_epochs=7,\n",
        "    per_device_train_batch_size=16,\n",
        "    warmup_steps=1000,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    fp16=True,\n",
        "    )\n",
        "\n",
        "# Re-initialize and train the Trainer with the new combined dataset\n",
        "final_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=final_dataset,\n",
        "    compute_metrics=None\n",
        ")\n",
        "\n",
        "# Retrain the model on the whole dataset\n",
        "final_trainer.train()\n",
        "\n",
        "# Save the final trained model and tokenizer\n",
        "model.save_pretrained('./camembert_final_model')\n",
        "tokenizer.save_pretrained('./camembert_final_model')"
      ],
      "metadata": {
        "id": "uHUdB4QEouy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the unlabelled data\n",
        "url = 'https://raw.githubusercontent.com/RebeccaKessler/Machine_Learning/main/unlabelled_test_data.csv'\n",
        "unlabelled_data = pd.read_csv(url)\n",
        "\n",
        "# Load the saved CamemBERT model and tokenizer\n",
        "camembert_model_path = './camembert_final_model'\n",
        "camembert_model = CamembertForSequenceClassification.from_pretrained(camembert_model_path)\n",
        "camembert_tokenizer = CamembertTokenizer.from_pretrained(camembert_model_path)\n",
        "\n",
        "# Load the saved Flaubert model and tokenizer\n",
        "flaubert_model_path = './flaubert_final_model'\n",
        "flaubert_model = FlaubertForSequenceClassification.from_pretrained(flaubert_model_path)\n",
        "flaubert_tokenizer = FlaubertTokenizer.from_pretrained(flaubert_model_path)\n",
        "\n",
        "# Create prediction pipelines to get probabilities\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "camembert_classifier = pipeline('text-classification', model=camembert_model, tokenizer=camembert_tokenizer, framework='pt', device=device, return_all_scores=True)\n",
        "flaubert_classifier = pipeline('text-classification', model=flaubert_model, tokenizer=flaubert_tokenizer, framework='pt', device=device, return_all_scores=True)\n",
        "\n",
        "# Predict probabilities for the unlabelled data using both models\n",
        "camembert_probs = camembert_classifier(unlabelled_data['sentence'].tolist())\n",
        "flaubert_probs = flaubert_classifier(unlabelled_data['sentence'].tolist())\n",
        "\n",
        "# Convert the predictions to numpy arrays\n",
        "camembert_probs_array = np.array([[prob['score'] for prob in probs] for probs in camembert_probs])\n",
        "flaubert_probs_array = np.array([[prob['score'] for prob in probs] for probs in flaubert_probs])\n",
        "\n",
        "# Combine predictions using soft voting (average probabilities)\n",
        "average_probs = (camembert_probs_array + flaubert_probs_array) / 2\n",
        "final_predictions = np.argmax(average_probs, axis=1)\n",
        "\n",
        "# Decode the numeric labels to original labels using the loaded LabelEncoder\n",
        "predicted_labels = label_encoder.inverse_transform(final_predictions)\n",
        "\n",
        "# Create a DataFrame to export\n",
        "results_df = pd.DataFrame({\n",
        "    'id': unlabelled_data['id'],\n",
        "    'difficulty': predicted_labels\n",
        "})\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "results_df.to_csv('predicted_difficulties.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'predicted_difficulties.csv'\")\n"
      ],
      "metadata": {
        "id": "7RipUgc2mFkc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}