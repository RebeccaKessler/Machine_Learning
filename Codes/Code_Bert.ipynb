{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RebeccaKessler/Machine_Learning/blob/main/Codes/Code_Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy matplotlib\n",
        "!pip install scikit-learn seaborn"
      ],
      "metadata": {
        "id": "lQdp6hW04H7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "h9fR8W0AHLz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_training_data = pd.read_csv('training_data.csv')\n",
        "df_unlabelled_test_data = pd.read_csv('unlabelled_test_data.csv')"
      ],
      "metadata": {
        "id": "4qgB7G6YHNUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_training_data.head(5)"
      ],
      "metadata": {
        "id": "29_bsVRjMHnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unlabelled_test_data.head()"
      ],
      "metadata": {
        "id": "3xG0VB8IzHZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert Model"
      ],
      "metadata": {
        "id": "Kl830xOoGpFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "_G8d5sdAGrgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, get_scheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "bFncpzco4-Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a mapping from string labels to integers\n",
        "label_mapping = {\n",
        "    'A1': 0,\n",
        "    'A2': 1,\n",
        "    'B1': 2,\n",
        "    'B2': 3,\n",
        "    'C1': 4,\n",
        "    'C2': 5\n",
        "}"
      ],
      "metadata": {
        "id": "BTJjopzB5Aog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FrenchDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, tokenizer, max_len=512):\n",
        "        self.sentences = sentences\n",
        "        self.labels = [label_mapping[label] for label in labels]  # Convert string labels to integers\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(sentence, add_special_tokens=True, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "Rq2hvMuF5Dxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset\n",
        "df = pd.read_csv('training_data.csv')\n",
        "sentences = df['sentence'].tolist()\n",
        "labels = df['difficulty'].tolist()\n",
        "\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = FrenchDataset(train_sentences, train_labels, tokenizer)\n",
        "val_dataset = FrenchDataset(val_sentences, val_labels, tokenizer)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "#Prepare the model\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased', num_labels=6)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Step 3: Set up training\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 4\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ],
      "metadata": {
        "id": "0hu6R4wc5cRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Training loop\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "-xdPuguh5hmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"your_model_directory\")\n",
        "tokenizer.save_pretrained(\"your_model_directory\")"
      ],
      "metadata": {
        "id": "rvf8KQ7fE-Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            _, predicted_labels = torch.max(logits, dim=1)\n",
        "\n",
        "            predictions.extend(predicted_labels.cpu().numpy())\n",
        "            true_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "evaluation_results = evaluate_model(model, val_loader, device)\n",
        "print(evaluation_results)"
      ],
      "metadata": {
        "id": "3_8SRqo4FBxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "# Create a dataset with all data\n",
        "full_dataset = FrenchDataset(sentences, labels, tokenizer)\n",
        "\n",
        "# Data loader for the full dataset\n",
        "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased', num_labels=6)\n",
        "model.to(device)\n",
        "\n",
        "#set up training\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 4\n",
        "num_training_steps = num_epochs * len(full_loader)\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ],
      "metadata": {
        "id": "TMpInN08FDbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in full_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "b2q3jJSZFIT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"full_model_directory\")\n",
        "tokenizer.save_pretrained(\"full_model_directory\")"
      ],
      "metadata": {
        "id": "5BfGXN-ZFKOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnlabeledFrenchDataset(Dataset):\n",
        "    def __init__(self, sentences, tokenizer, max_len=512):\n",
        "        self.sentences = sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        encoding = self.tokenizer(sentence, add_special_tokens=True, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }"
      ],
      "metadata": {
        "id": "pnWmU7R0O4eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data\n",
        "unlabeled_df = pd.read_csv('unlabelled_test_data.csv')\n",
        "unlabeled_sentences = unlabeled_df['sentence'].tolist()"
      ],
      "metadata": {
        "id": "RVPoKYkWFLec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare unlabeled dataset\n",
        "unlabeled_dataset = UnlabeledFrenchDataset(unlabeled_sentences, tokenizer)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "yAY4mlqDPOmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in unlabeled_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        _, predicted_labels = torch.max(logits, dim=1)\n",
        "        predictions.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "# Map predictions back to label strings\n",
        "predicted_difficulties = [list(label_mapping.keys())[label] for label in predictions]\n",
        "\n",
        "# Combine predictions with sentences\n",
        "data = {\n",
        "    'sentence': unlabeled_sentences,\n",
        "    'predicted_difficulty': predicted_difficulties\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "results_df = pd.DataFrame(data)\n",
        "\n",
        "# Save to CSV\n",
        "results_df.to_csv('/mnt/data/predicted_difficulties.csv', index=False)"
      ],
      "metadata": {
        "id": "rRbn6AyuPRq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Camabert Model"
      ],
      "metadata": {
        "id": "SrsiaI6BM7pS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 1 (simple)"
      ],
      "metadata": {
        "id": "LSRipUbZPNZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "d7rvu1BbSIV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sentencepiece as spm\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "l9ecx3cuR1Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FrenchDifficultyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "        return len(self.labels)\n"
      ],
      "metadata": {
        "id": "f-1YcEl7M8nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('training_data.csv')\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder and return encoded labels\n",
        "data['encoded_labels'] = label_encoder.fit_transform(data['difficulty'])\n",
        "\n",
        "# Now split the dataset with encoded labels\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    data['sentence'], data['encoded_labels'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Reset index to ensure proper alignment and avoid KeyError\n",
        "train_labels = train_labels.reset_index(drop=True)\n",
        "val_labels = val_labels.reset_index(drop=True)\n",
        "\n",
        "\n",
        "from transformers import CamembertTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
        "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n",
        "\n",
        "train_dataset = FrenchDifficultyDataset(train_encodings, train_labels)\n",
        "val_dataset = FrenchDifficultyDataset(val_encodings, val_labels)"
      ],
      "metadata": {
        "id": "b-4_2dwbP4uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CamembertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Load CamemBERT model pre-trained\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "\n",
        "# Define compute metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(-1)\n",
        "    return {'accuracy': accuracy_score(labels, predictions)}\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    gradient_accumulation_steps=2,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "trainer.train()\n",
        "evaluation_results = trainer.evaluate()\n",
        "print(evaluation_results)"
      ],
      "metadata": {
        "id": "txNWc8LiQKAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "fdfmbtZj8sJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "Ywx6xlNR8mZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "KFEEiFdEW5aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments, CamembertConfig\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch"
      ],
      "metadata": {
        "id": "7Haoxxrq8rgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = str(self.data.iloc[idx]['sentence'])\n",
        "        label = int(self.data.iloc[idx]['encoded_labels'])\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "-WjmIHWZ8029"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('training_data.csv')\n",
        "\n",
        "# Define label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder and transform labels\n",
        "data['encoded_labels'] = label_encoder.fit_transform(data['difficulty'])\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a function to train and evaluate the model with given hyperparameters\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters to tune\n",
        "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 3, 7)\n",
        "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32])\n",
        "\n",
        "    model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "\n",
        "    tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        learning_rate=1e-5,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        warmup_steps=1000,\n",
        "        weight_decay=0.1,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        gradient_accumulation_steps=2,\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=CustomDataset(train_data, tokenizer),\n",
        "        eval_dataset=CustomDataset(val_data, tokenizer),\n",
        "        compute_metrics=lambda p: {'accuracy': accuracy_score(p.predictions.argmax(axis=1), p.label_ids)}\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_result = trainer.evaluate()\n",
        "\n",
        "    return eval_result[\"eval_accuracy\"]\n",
        "\n",
        "# Perform hyperparameter optimization\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial.params)"
      ],
      "metadata": {
        "id": "lvxlyPII86Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## version 2 (with optimized parameters)"
      ],
      "metadata": {
        "id": "mpCfagQDNaQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "kSl56l2dcdgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments, CamembertConfig\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "Tn3OPjCI-9z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = str(self.data.iloc[idx]['sentence'])\n",
        "        label = int(self.data.iloc[idx]['encoded_labels'])\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "6mZLVvvR_zig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('training_data.csv')\n",
        "\n",
        "# Define label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder and transform labels\n",
        "data['encoded_labels'] = label_encoder.fit_transform(data['difficulty'])\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "gBcW6TU5_3mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define compute metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(-1)\n",
        "    return {'accuracy': accuracy_score(labels, predictions)}"
      ],
      "metadata": {
        "id": "b90oKuS6Hb1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CamembertConfig, CamembertForSequenceClassification, CamembertTokenizer, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "IwjSGqFi5fgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.00019389784400015783\n",
        "num_train_epochs = 5\n",
        "per_device_train_batch_size = 32\n",
        "\n",
        "# Modify training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=learning_rate,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    warmup_steps=1000,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    gradient_accumulation_steps=2,\n",
        "    fp16=True)\n",
        "\n",
        "# Load CamemBERT model pre-trained\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "train_dataset = CustomDataset(train_data, tokenizer)\n",
        "eval_dataset = CustomDataset(val_data, tokenizer)\n",
        "\n",
        "# Re-initialize and train the Trainer with possibly adjusted datasets\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=lambda p: {'accuracy': accuracy_score(p.predictions.argmax(axis=1), p.label_ids)}\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "eval_result = trainer.evaluate()\n",
        "print(eval_result)"
      ],
      "metadata": {
        "id": "TAStEFgTPjIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrain on extended dataset"
      ],
      "metadata": {
        "id": "wtVbGFUlb9AN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_data = pd.read_csv('combined_random_french_sentences.csv')\n",
        "\n",
        "# Define and fit label encoder, transform labels\n",
        "label_encoder = LabelEncoder()\n",
        "full_data['encoded_labels'] = label_encoder.fit_transform(full_data['difficulty'])\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "full_dataset = CustomDataset(full_data, tokenizer)"
      ],
      "metadata": {
        "id": "bgJF0XLyF0Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=0.00019389784400015783,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=32,\n",
        "    warmup_steps=1000,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    gradient_accumulation_steps=2,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"no\"  # Disable evaluation\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=full_dataset,  # use the full dataset for training\n",
        "    compute_metrics=None  # Optional: Define this function if you want metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "fmmP7oluGdvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test on unlabeled data"
      ],
      "metadata": {
        "id": "3hHRE4w7cIfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('unlabelled_test_data.csv')"
      ],
      "metadata": {
        "id": "5NhX1cOKGpB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = str(self.data.iloc[idx]['sentence'])\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }"
      ],
      "metadata": {
        "id": "_QY2qBAJG0n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test dataset (without labels)\n",
        "test_dataset = TestDataset(test_data, tokenizer)\n",
        "\n",
        "# Make predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Decode predictions to labels\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "predicted_labels = label_encoder.inverse_transform(predicted_labels)\n"
      ],
      "metadata": {
        "id": "OAeeOePrG3UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df = pd.DataFrame({\n",
        "    'id': test_data['id'],\n",
        "    'difficulty': predicted_labels\n",
        "})\n",
        "\n",
        "output_df.to_csv('submissions_camabert_without_dropout_random.csv', index=False)"
      ],
      "metadata": {
        "id": "43FoJy2LG5X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 3 (with transfer learning)"
      ],
      "metadata": {
        "id": "4TkInhfxcK5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate -U\n",
        "!pip install pandas numpy matplotlib\n",
        "!pip install scikit-learn seaborn"
      ],
      "metadata": {
        "id": "0jnt6ZeKcS5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load data from CSV file\n",
        "df = pd.read_csv(\"training_data.csv\")\n",
        "\n",
        "# Assuming your dataset has 'sentence' and 'difficulty' columns\n",
        "sentences = df['sentence'].tolist()\n",
        "difficulties = df['difficulty'].tolist()\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(sentences, difficulties, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained CamemBERT tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "# Tokenize training and validation texts\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder and transform labels\n",
        "train_labels = label_encoder.fit_transform(train_labels)\n",
        "val_labels = label_encoder.transform(val_labels)\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "\n",
        "# Create datasets\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "val_dataset = CustomDataset(val_encodings, val_labels)\n",
        "\n",
        "# Fine-tune CamemBERT for sequence classification\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "optimizer = AdamW(model.parameters(), lr=20e-5)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    gradient_accumulation_steps=2\n",
        ")\n",
        "\n",
        "# Define a function to compute accuracy\n",
        "def compute_accuracy(p):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    return {\"accuracy\": accuracy_score(preds, p.label_ids)}\n",
        "\n",
        "# Define the total number of training steps\n",
        "total_steps = len(train_dataset) * training_args.num_train_epochs\n",
        "\n",
        "# Create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=500,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Update the 'optimizers' argument in Trainer initialization\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_accuracy,\n",
        "    optimizers=(optimizer, scheduler),  # Include the scheduler\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "train_history = trainer.train()\n",
        "\n",
        "# Evaluate the fine-tuned model\n",
        "eval_results = trainer.evaluate(eval_dataset=val_dataset)\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "FQHz4N7IcXNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")"
      ],
      "metadata": {
        "id": "qn52LLDoccEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "model = CamembertForSequenceClassification.from_pretrained('./fine_tuned_model')\n",
        "tokenizer = CamembertTokenizer.from_pretrained('./fine_tuned_model')\n",
        "\n",
        "# Load your unlabeled dataset (assuming it's stored in a CSV file)\n",
        "unlabeled_df = pd.read_csv(\"unlabelled_data.csv\")\n",
        "\n",
        "# Tokenize the sentences in the unlabeled dataset\n",
        "tokenized_texts = tokenizer(unlabeled_df['sentence'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "# Create torch dataset\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "unlabeled_dataset = CustomDataset(tokenized_texts)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Generate predictions for the unlabeled dataset\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in torch.utils.data.DataLoader(unlabeled_dataset, batch_size=32):\n",
        "        input_ids = batch['input_ids'].to(model.device)\n",
        "        attention_mask = batch['attention_mask'].to(model.device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "# Add predictions to the original DataFrame\n",
        "unlabeled_df['difficulty'] = predictions\n",
        "\n",
        "# Export predictions to a CSV file\n",
        "unlabeled_df[['id', 'difficulty']].to_csv(\"predictions.csv\", index=False)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract training loss and evaluation metrics from train_history\n",
        "train_loss_values = train_history['loss']\n",
        "eval_accuracy_values = trainer['eval_accuracy']\n",
        "\n",
        "# Plot the learning curve\n",
        "plt.plot(range(1, len(train_loss_values) + 1), train_loss_values, label='Training Loss')\n",
        "plt.plot(range(1, len(eval_accuracy_values) + 1), eval_accuracy_values, label='Evaluation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.title('Learning Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iDPWDjZ6cekQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}